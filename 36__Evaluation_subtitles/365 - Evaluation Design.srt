1
00:00:00,270 --> 00:00:03,660
 Regardless of the type of evaluation you're planning to perform, there's

2
00:00:03,660 --> 00:00:07,990
 a series of steps to perform to ensure that the evaluation is actually useful.

3
00:00:07,990 --> 00:00:11,071
 First, we want to clearly define the task that we're examining.

4
00:00:11,071 --> 00:00:14,300
 Depending on your place in the design process this can be very large or

5
00:00:14,300 --> 00:00:15,380
 very small.

6
00:00:15,380 --> 00:00:18,980
 If we were designing Facebook, it can be as simple as posting a status update, or

7
00:00:18,980 --> 00:00:22,550
 as complicated as navigating amongst and using several different pages.

8
00:00:22,550 --> 00:00:25,620
 It could involve context and constraints like taking notes while running, or

9
00:00:25,620 --> 00:00:28,630
 looking up a restaurant address without touching the screen.

10
00:00:28,630 --> 00:00:29,580
 Whatever it is,

11
00:00:29,580 --> 00:00:33,910
 we want to start by clearly identifying what task we're going to investigate.

12
00:00:33,910 --> 00:00:37,095
 Second, we want to define our performance measures.

13
00:00:37,095 --> 00:00:39,835
 How are we going to evaluate the user's performance?

14
00:00:39,835 --> 00:00:41,865
 Qualitatively, it could be based on their spoken or

15
00:00:41,865 --> 00:00:44,055
 written feedback about the experience.

16
00:00:44,055 --> 00:00:47,365
 Quantitatively, we can measure efficiency in certain activities or

17
00:00:47,365 --> 00:00:49,135
 count the number of mistakes.

18
00:00:49,135 --> 00:00:52,885
 Defining performance measures helps us avoid confirmation bias.

19
00:00:52,885 --> 00:00:55,515
 It makes sure we don't just pick out whatever observations or

20
00:00:55,515 --> 00:00:59,320
 data confirm our hypotheses, or say that we have a good interface.

21
00:00:59,320 --> 00:01:02,520
 It forces us to look at it objectively.

22
00:01:02,520 --> 00:01:05,019
 Third, we develop the experiment.

23
00:01:05,019 --> 00:01:08,040
 How will we find user's performance on the performance measures?

24
00:01:08,040 --> 00:01:10,940
 If we're looking qualitatively will we have them think out loud while

25
00:01:10,940 --> 00:01:11,740
 they're using the tool?

26
00:01:11,740 --> 00:01:14,400
 Or will we have them do a survey after they're done?

27
00:01:14,400 --> 00:01:17,420
 If we're looking quantitatively what will we measure, what will we control,

28
00:01:17,420 --> 00:01:18,910
 and what will we vary?

29
00:01:18,910 --> 00:01:21,590
 This is also where we ask questions about whether our assessment

30
00:01:21,590 --> 00:01:23,750
 measures are reliable and valid.

31
00:01:23,750 --> 00:01:26,580
 And whether the users we're testing are generalizable.

32
00:01:26,580 --> 00:01:29,140
 Fourth, we recruit the participants.

33
00:01:29,140 --> 00:01:32,310
 As part of the ethics process, we make sure we're recruiting participants

34
00:01:32,310 --> 00:01:35,510
 who are aware of their rights and contributing willingly.

35
00:01:35,510 --> 00:01:37,500
 Then fifth, we do the experiment.

36
00:01:37,500 --> 00:01:41,590
 We have them walk-through what we outline when we develop the experiment.

37
00:01:41,590 --> 00:01:43,580
 Sixth, we analyze the data.

38
00:01:43,580 --> 00:01:47,820
 We focus on what the data tells us about our performance measures.

39
00:01:47,820 --> 00:01:50,730
 It's important that we stay close to what we outlined initially.

40
00:01:50,730 --> 00:01:53,740
 It can be tempting to just look for whatever supports are design but

41
00:01:53,740 --> 00:01:55,680
 we want to be impartial.

42
00:01:55,680 --> 00:01:58,940
 If we find some evidence that suggests our interface is good in ways we didn't

43
00:01:58,940 --> 00:02:03,315
 anticipate, we can always do a follow up experiment to test if we're right.

44
00:02:03,315 --> 00:02:06,345
 Seventh, we summarize the data in a way that informs our on

45
00:02:06,345 --> 00:02:08,264
 going design process.

46
00:02:08,264 --> 00:02:09,485
 What did our data say was working?

47
00:02:09,485 --> 00:02:11,115
 What could be improved?

48
00:02:11,115 --> 00:02:13,345
 How can we take the results of this experiment and

49
00:02:13,345 --> 00:02:16,125
 use it to then revise our interface?

50
00:02:16,125 --> 00:02:20,435
 The results of this experiment then become part of our design life cycle.

51
00:02:20,435 --> 00:02:25,480
 We investigated user needs, develop alternatives, made a prototype and

52
00:02:25,480 --> 00:02:27,810
 put the prototype in front of users.

53
00:02:27,810 --> 00:02:30,450
 To put the prototype in front of users,

54
00:02:30,450 --> 00:02:32,320
 we walked through this experimental method.

55
00:02:32,320 --> 00:02:34,470
 We defined the task, defined the performance measures,

56
00:02:34,470 --> 00:02:37,960
 developed the experiment, recruited them, did the experiment,

57
00:02:37,960 --> 00:02:41,250
 analyzed our data and summarized our data.

58
00:02:41,250 --> 00:02:44,980
 Based on the experience, we now have the data necessary to develop a better

59
00:02:44,980 --> 00:02:49,300
 understanding of the user's needs, to revisit our earlier design alternatives

60
00:02:49,300 --> 00:02:52,710
 and to either improve our prototypes by increasing their fidelity or

61
00:02:52,710 --> 00:02:55,270
 by revising them based on what we just learned.

62
00:02:55,270 --> 00:02:57,800
 Regardless of whether we're doing qualitative, empirical, or

63
00:02:57,800 --> 00:03:02,030
 predictive evaluation, these steps remain largely the same.

64
00:03:02,030 --> 00:03:06,130
 Those different types of evaluation just fill in the experiment that we develop,

65
00:03:06,130 --> 00:03:06,130
 and they inform our performance measure, data analysis, and summaries.

